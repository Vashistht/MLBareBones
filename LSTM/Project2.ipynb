{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e2a29d",
   "metadata": {},
   "source": [
    "1. Make sure you fill in all cells contain YOUR CODE HERE or YOUR ANSWER HERE.\n",
    "2. After you finished, Restart the kernel & run all cell in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07553a",
   "metadata": {},
   "source": [
    "# Project II: Text Classification Using LSTM Network\n",
    "## Deadline: Nov 14, 11:59 pm\n",
    "\n",
    "You have learned about the basics of neural network training and testing during the class. Now let's move forward to the text classification tasks using simple LSTM networks! In this project, you need to implement two parts:\n",
    "\n",
    "- **Part I: Building vocabulary for LSTM network**\n",
    "    - Get familiar with discrete text data processing for neural networks. Building vocabulary by yourself.\n",
    "\n",
    "\n",
    "- **Part II: Implementing your own LSTM Neural Network**\n",
    "    - Learn to implement your own LSTM network and aims for a strong performance on the given text classification task.\n",
    "    - Note that you need to implement the LSTM network manually, any kind of integrated package invoking will get 0 points.\n",
    "    - Your LSTM network can be 2-4 layers.\n",
    "    - Expected Accuracy: >=65%.\n",
    "    ![](./LSTM.png)\n",
    "    \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbbc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn # for checking\n",
    "\n",
    "# nlp library of Pytorch\n",
    "from torchtext import data\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "SEED = 2021\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cuda.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bfaa96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    5574 non-null   object\n",
      " 1   text    5574 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_ = pd.read_csv('./sms_spam.csv')\n",
    "data_.head()\n",
    "data_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8c9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field is a normal column \n",
    "# LabelField is the label column.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [ tok.text for tok in nlp.tokenizer(text) ]\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenizer,batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aac86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"type\",LABEL),('text',TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866b193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'ham', 'text': ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']}\n"
     ]
    }
   ],
   "source": [
    "training_data = data.TabularDataset(path=\"./sms_spam.csv\",\n",
    "                                    format=\"csv\",\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True\n",
    "                                   )\n",
    "\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5945e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# train and validation splitting\n",
    "train_data,valid_data = training_data.split(split_ratio=0.75,\n",
    "                                            random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e02735",
   "metadata": {},
   "source": [
    "#### Question 1 (5 points)\n",
    "Implement the vocabulary building and the text to label part for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4505d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Question1 here:\n",
    "#Building vocabularies => (Token to integer)\n",
    "#you can use the data package built-in function to build the vocabulary, check the 'torchtext data' doc.\n",
    "TEXT.build_vocab(train_data, min_freq = 3)\n",
    "LABEL.build_vocab(train_data, min_freq= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80216804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text vocab: 2820\n",
      "Size of label vocab: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 3658),\n",
       " ('to', 1615),\n",
       " ('I', 1478),\n",
       " (',', 1461),\n",
       " ('you', 1383),\n",
       " ('?', 1086),\n",
       " ('!', 1019),\n",
       " ('a', 1003),\n",
       " ('the', 882),\n",
       " ('...', 869)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Size of text vocab:\",len(TEXT.vocab))\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e3e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\") # had problems running on gpu\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# We'll create iterators to get batches of data when we want to use them\n",
    "\"\"\"\n",
    "This BucketIterator batches the similar length of samples and reduces the need of \n",
    "padding tokens. This makes our future model more stable\n",
    "\"\"\"\n",
    "\n",
    "train_iterator,validation_iterator = data.BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Sort key is how to sort the samples\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acad1a",
   "metadata": {},
   "source": [
    "#### Question 2 (25 points)\n",
    "You need to implement the embedding layer and the LSTM cell according to the given architecture, but you are not allowed to use any integrated package!\n",
    "LSTM tutorial: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "![](./LSTM_CELL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa99072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acac77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a single GRU cell as given in the tutorial\n",
    "class GRULayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "       \n",
    "        super(GRULayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.W_z = nn.Linear(self.input_dim + self.hidden_dim , self.hidden_dim)\n",
    "        self.W_r = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        self.W = nn.Linear(self.input_dim   + self.hidden_dim, self.hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # self.bidirectional = bidrectional\n",
    "\n",
    "    def forward(self, x_input, input_cell):\n",
    "\n",
    "        batch, sequence, _  = x_input.shape\n",
    "        hidden_sequence = []\n",
    "        \n",
    "        if input_cell is None:\n",
    "            h_t_minus1 = torch.zeros(batch, self.hidden_dim).to(x_input.device)\n",
    "        else:\n",
    "            h_t_minus1 = input_cell \n",
    "        \n",
    "        for t in range(sequence):\n",
    "            x_t = x_input[:, t, :]\n",
    "            z_t = self.sigmoid(self.W_z((torch.cat((h_t_minus1, x_t), dim=-1) ))) # W_z dot [h_t-1, x_t]\n",
    "            r_t = self.sigmoid(self.W_r((torch.cat((h_t_minus1, x_t), dim=-1) ))) # W_r dot [h_t-1, x_t]\n",
    "            h_tilda_t = self.tanh(self.W(torch.cat(((r_t * h_t_minus1), x_t), dim=-1)))  # W dot [r_t * h_t-1, x_t]\n",
    "            h_t = (1-z_t) * h_t_minus1 + z_t * h_tilda_t\n",
    "            h_t_minus1 = h_t # update h_t to be the new h_t_minus1 \n",
    "            h_t = h_t.unsqueeze(0)\n",
    "            hidden_sequence.append(h_t)\n",
    "        \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "        hidden_sequence = hidden_sequence.transpose(0,1).contiguous()\n",
    "        \n",
    "        return hidden_sequence, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69493168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "        \n",
    "        super(LSTMNet,self).__init__()\n",
    "        # In this class, you need to implement the architecture of an LSTM network, the architecture should include:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 1. Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 2. LSTM layer process the vector sequences \n",
    "        self.layered_gru = nn.ModuleList()\n",
    "        \n",
    "        for i in range(0, self.num_layers):\n",
    "            if i == 0:\n",
    "                self.layered_gru.append( GRULayer(input_dim = self.input_dim, hidden_dim= self.hidden_dim) )\n",
    "            else:\n",
    "                self.layered_gru.append( GRULayer(input_dim = self.hidden_dim, hidden_dim= self.hidden_dim) )\n",
    "        \n",
    "        # 3. Dense layer to predict \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "        # 4. Prediction activation function (you can choose your own activate function e.g., ReLU, Sigmoid, Tanh)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        text_embedding = self.embedding_layer(text)             #embedding input\n",
    "        \n",
    "        if self.bidirectional is True:\n",
    "            \n",
    "            for (l, gru) in enumerate(self.layered_gru):\n",
    "                input_cell = None\n",
    "                if l == 0:\n",
    "                    output_forward, input_cell_forward = gru.forward(x_input = text_embedding, input_cell = input_cell)\n",
    "                    output_backward, input_cell_backward = gru.forward(x_input = text_embedding.flip([0,1]), input_cell = input_cell)\n",
    "                    \n",
    "                    output = output_forward + output_backward\n",
    "                else:\n",
    "                    output_forward, input_cell_forward = gru.forward(x_input = output, input_cell = input_cell)\n",
    "                    output_backward, input_cell_backward = gru.forward(x_input = output_forward.flip([0,1]), input_cell = input_cell)\n",
    "\n",
    "        else:\n",
    "            for (l, gru) in enumerate(self.layered_gru):\n",
    "                input_cell = None\n",
    "                if l == 0:\n",
    "                    output, input_cell = gru.forward(x_input = text_embedding, input_cell = input_cell)\n",
    "                else:\n",
    "                    output, input_cell = gru.forward(x_input = output, input_cell = input_cell)\n",
    "    \n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1937b5e",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "888e86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HIDDEN_NODES = 64\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e01a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(SIZE_OF_VOCAB,\n",
    "                EMBEDDING_DIM,\n",
    "                NUM_HIDDEN_NODES,\n",
    "                NUM_OUTPUT_NODES,\n",
    "                NUM_LAYERS,\n",
    "                BIDIRECTION,\n",
    "                DROPOUT\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5235eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd1406ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79f86a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # cleaning the cache of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text,text_lengths = batch.text\n",
    "        \n",
    "        # forward propagation and squeezing\n",
    "        predictions = model(text,text_lengths).squeeze()\n",
    "        \n",
    "        # computing loss / backward propagation\n",
    "        loss = criterion(predictions,batch.type)\n",
    "        loss.backward()\n",
    "        \n",
    "        # accuracy\n",
    "        acc = binary_accuracy(predictions,batch.type)\n",
    "        \n",
    "        # updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # It'll return the means of loss and accuracy\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f6898cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    # deactivate the dropouts\n",
    "    model.eval()\n",
    "    \n",
    "    # Sets require_grad flat False\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text,text_lengths).squeeze()\n",
    "              \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.type)\n",
    "            acc = binary_accuracy(predictions, batch.type)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f93e785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.544 | Train Acc: 81.13%\n",
      "\t Val. Loss: 0.413 |  Val. Acc: 88.12%\n",
      "\n",
      "\tTrain Loss: 0.417 | Train Acc: 86.32%\n",
      "\t Val. Loss: 0.370 |  Val. Acc: 88.26%\n",
      "\n",
      "\tTrain Loss: 0.375 | Train Acc: 86.58%\n",
      "\t Val. Loss: 0.342 |  Val. Acc: 88.33%\n",
      "\n",
      "\tTrain Loss: 0.344 | Train Acc: 86.67%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 88.62%\n",
      "\n",
      "\tTrain Loss: 0.321 | Train Acc: 87.69%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 88.76%\n",
      "\n",
      "\tTrain Loss: 0.292 | Train Acc: 88.28%\n",
      "\t Val. Loss: 0.272 |  Val. Acc: 89.18%\n",
      "\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.90%\n",
      "\t Val. Loss: 0.250 |  Val. Acc: 90.39%\n",
      "\n",
      "\tTrain Loss: 0.250 | Train Acc: 89.75%\n",
      "\t Val. Loss: 0.228 |  Val. Acc: 91.60%\n",
      "\n",
      "\tTrain Loss: 0.234 | Train Acc: 90.93%\n",
      "\t Val. Loss: 0.217 |  Val. Acc: 92.24%\n",
      "\n",
      "\tTrain Loss: 0.206 | Train Acc: 92.09%\n",
      "\t Val. Loss: 0.190 |  Val. Acc: 93.02%\n",
      "\n",
      "\tTrain Loss: 0.191 | Train Acc: 93.30%\n",
      "\t Val. Loss: 0.167 |  Val. Acc: 94.51%\n",
      "\n",
      "\tTrain Loss: 0.167 | Train Acc: 94.48%\n",
      "\t Val. Loss: 0.151 |  Val. Acc: 95.65%\n",
      "\n",
      "\tTrain Loss: 0.149 | Train Acc: 94.89%\n",
      "\t Val. Loss: 0.159 |  Val. Acc: 95.54%\n",
      "\n",
      "\tTrain Loss: 0.134 | Train Acc: 95.64%\n",
      "\t Val. Loss: 0.143 |  Val. Acc: 95.98%\n",
      "\n",
      "\tTrain Loss: 0.126 | Train Acc: 96.12%\n",
      "\t Val. Loss: 0.128 |  Val. Acc: 96.14%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUMBER = 15\n",
    "\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    \n",
    "    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n",
    "    \n",
    "    valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n",
    "    \n",
    "    # Showing statistics\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('project2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5409ef177a33f2749e77ae560016e9f19ce5541861bf5be55811d157888cc9aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
